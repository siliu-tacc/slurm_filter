#
# (c) Copyright 2013 Cray Inc.  All Rights Reserved.
#
# This file was generated by slurmconfgen.py. Rather than editing this file
# directly, you may want to edit slurm.conf.template and run slurmconfgen.py
# again.
#
# See the slurm.conf man page for more information.
#
#------------------------------------------------------------------------------
# Note: this file is managed via LosF
#
# You should, in general, not edit this file directly as you will 
# *lose* the contents during the next sync process. Instead, 
# edit the template file in your local config directory:
# 
# $LOSF_CONFIG_DIR/const_files/lonestar5/login/slurm.conf
#
# Contact: minyard@tacc.utexas.edu or dgignac@tacc.utexas.edu
#------------------------------------------------------------------------------
#
ControlMachine=sdb
#ControlAddr=
#BackupController=
#BackupAddr=
#
#CryptoType=crypto/munge
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
ProctrackType=proctrack/cray
#Prolog=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=ALL
# Some programming models require unlimited virtual memory
# Configured like stampede
PropagateResourceLimitsExcept=MEMLOCK,NOFILE,NPROC,AS
#PropagateResourceLimitsExcept=AS,MEMLOCK
#RebootProgram=
# ReturnToService 2 will let rebooted nodes come back up immediately
ReturnToService=0
#SallocDefaultCommand=
SlurmctldPidFile=/var/spool/slurm/slurmctld.pid
SlurmctldPort=60817
SrunPortRange=60001-63000
SlurmdPidFile=/var/spool/slurmd/slurmd.pid
SlurmdPort=60818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=root
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm
SwitchType=switch/cray
#SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity,task/cgroup,task/cray
#TaskPluginParam=
#TaskProlog=
TopologyPlugin=topology/none
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=

# TIMERS
##modify TACC
#BatchStartTimeout=10
BatchStartTimeout=120
#MessageTimeout=10
MessageTimeout=90
#SlurmctldTimeout=120
SlurmctldTimeout=300
#CoreSpecPlugin=cray
#JobSubmitPlugins=cray


#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
ResvOverRun=720
MinJobAge=300
#OverTimeLimit=0
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
DefMemPerCPU=1337
FastSchedule=0
MaxMemPerCPU=64225
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cray
SelectTypeParameters=CR_CORE_Memory,other_cons_res
#
#
# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/basic
PriorityType=priority/multifactor
PriorityDecayHalfLife=2-0
#PriorityCalcPeriod=
#PriorityFavorSmall=
PriorityMaxAge=7-0
#PriorityUsageResetPeriod=
PriorityWeightAge=100
PriorityWeightFairshare=100
PriorityWeightJobSize=100
PriorityWeightPartition=100
#PriorityWeightQOS=
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
#AccountingStorageHost=sdb
#AccountingStorageLoc=/var/log/slurm/accounting
#AccountingStoragePass=
#AccountingStoragePort=
#AccountingStorageType=accounting_storage/filetxt
#AccountingStorageUser=
#AccountingStoreJobComment=YES
ClusterName=clogin79
#DebugFlags=
DebugFlags=NO_CONF_HASH
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
#JobCompType=jobcomp/none
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
JobRequeue=0

SlurmctldDebug=info
#SlurmctldDebug=7

SlurmctldLogFile=/var/spool/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/spool/slurmd/%h.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#

### dgignac modifications 
JobCompType=jobcomp/script
JobCompLoc=/etc/opt/slurm/tacc_job_completion.sh
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageLoc=/var/spool/slurm/accounting
AccountingStorageHost=sdb
#AccountingStoragePort=7031
MpiParams=ports=12000-12999
Prolog=/opt/apps/tacc/bin/slurm_prolog
Epilog=/opt/apps/tacc/bin/slurm_epilog
PrologFlags=Alloc

MaxArraySize=0

#LLNL instructions
#UsePAM=0
UsePAM=1

# POWER SAVE SUPPORT FOR IDLE NODES (optional)
CpuFreqDef=performance
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
# COMPUTE NODES
NodeName=nid0[0008-0067,0072-0127,0132-0151,0160-0183,0192-0195,0200-0255,0260-0323,0328-0383,0388-0451,0456-0511,0516-0579,0584-0639,0644-0707,0712-0767,0772-0835,0840-0895,0900-0963,0968-1023,1028-1091,1096-1343] Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Gres=craynetwork:4 # RealMemory=65536 (set by FastSchedule 0)
NodeName=nid00[152-159,184-191] Sockets=1 CoresPerSocket=10 ThreadsPerCore=2 Gres=craynetwork:4,gpu:1 # RealMemory=65536 (set by FastSchedule 0)
#NodeName=lgmem0[1-7] Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Gres=craynetwork:4 # RealMemory=529208016 (set by FastSchedule 0)
NodeName=nid0200[0-7] Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Gres=craynetwork:4 # RealMemory=529208016 (set by FastSchedule 0)
NodeName=nid0200[8-9] Sockets=4 CoresPerSocket=12 ThreadsPerCore=2 Gres=craynetwork:4 # RealMemory=1058717528 (set by FastSchedule 0)

PartitionName=systest Nodes=nid0[0008-0067,0072-0127,0132-0151,0160-0183,0192-0195,0200-0255,0260-0323,0328-0383,0388-0451,0456-0511,0516-0579,0584-0639,0644-0707,0712-0767,0772-0835,0840-0895,0900-0963,0968-1023,1028-1091,1096-1342] Shared=EXCLUSIVE Priority=10 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=normal Nodes=nid0[0032-0067,0072-0127,0132-0151,0160-0183,0192-0195,0200-0255,0260-0323,0328-0383,0388-0451,0456-0511,0516-0579,0584-0639,0644-0707,0712-0767,0772-0835,0840-0895,0900-0963,0968-1023,1028-1091,1096-1328] Shared=EXCLUSIVE Priority=5 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=large Nodes=nid0[0032-0067,0072-0127,0132-0151,0160-0183,0192-0195,0200-0255,0260-0323,0328-0383,0388-0451,0456-0511,0516-0579,0584-0639,0644-0707,0712-0767,0772-0835,0840-0895,0900-0963,0968-1023,1028-1091,1096-1328] Shared=EXCLUSIVE Priority=5 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=development Nodes=nid0[0008-0031] Shared=EXCLUSIVE Priority=2 Default=YES DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=gpu Nodes=nid00[152-159,184-191] Shared=EXCLUSIVE Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=vis Nodes=nid00[152-159,184-191] Shared=EXCLUSIVE Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=grace-serial Nodes=nid01343 Shared=YES Hidden=YES Priority=5 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=grace Nodes=nid0[1329-1342] Shared=EXCLUSIVE Priority=5 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

#PartitionName=lgmem Nodes=lgmem0[0-7] Shared=EXCLUSIVE Hidden=YES Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN
PartitionName=lgmem Nodes=nid0200[0-7] Shared=EXCLUSIVE Hidden=YES Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN
#PartitionName=lgrmem Nodes=lgmem0[8-9] Shared=EXCLUSIVE Hidden=YES Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN
PartitionName=lgrmem Nodes=nid0200[8-9] Shared=EXCLUSIVE Hidden=YES Priority=1 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN

PartitionName=itar Nodes=nid0[0032-0067,0072-0127,0132-0151,0160-0183,0192-0195,0200-0255,0260-0323,0328-0383,0388-0451,0456-0511,0516-0579,0584-0639,0644-0707,0712-0767,0772-0835,0840-0895,0900-0963,0968-1023,1028-1091,1096-1328] Shared=EXCLUSIVE Hidden=YES Priority=5 DefaultTime=10:00 MaxTime=120:00:00 State=DOWN
